\documentclass[10p]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[backend=biber,%
            style=numeric-comp,     % how it appears in the literature overview
            citestyle=numeric-comp, % how it appears when citing in the text 
            maxnames=1]{biblatex}
\addbibresource{bibliography.bib} %Imports bibliography file

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}  % only number equations that have been references in the text (set 'false' to number all)

\usepackage[framemethod=TikZ]{mdframed}
\usepackage{xcolor}


\usepackage{booktabs}
\usepackage{url} 
\usepackage{adjustbox}
\usepackage[ruled,linesnumbered]{algorithm2e}

\usepackage{pdflscape}
\usepackage{hyperref}
\usepackage{tabularx}

\frenchspacing % No double spacing between sentences
\linespread{1.2} % Set linespace
\usepackage[a4paper, lmargin=0.1666\paperwidth, rmargin=0.1666\paperwidth, tmargin=0.1111\paperheight, bmargin=0.1111\paperheight]{geometry} %margins


\newcommand{\st}{\text{s.t.}}
\newcommand{\defi}{:=}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bones}{\mathbf{1}}
\newcommand{\numexp}{n}
\newcommand{\numpair}{m}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\Pset}{\mathcal{P}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\Molspace}{\mathcal{M}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\molkern}{\kappa}
\newcommand{\bmolkern}{\mathbf{k}}
\newcommand{\Molkern}{\mathbf{K}}
\newcommand{\syskern}{\lambda}
\DeclareMathOperator{\sign}{sign}


\mdfdefinestyle{codeframe}{%
    linecolor=black,
    outerlinewidth=0.5pt,
    roundcorner=0pt,
    innertopmargin=\baselineskip,
    innerbottommargin=\baselineskip,
    innerrightmargin=20pt,
    innerleftmargin=20pt,
    backgroundcolor=gray!20!white}
    
\mdfdefinestyle{openquestion}{%
    linecolor=black,
    outerlinewidth=0.5pt,
    roundcorner=0pt,
    innertopmargin=\baselineskip,
    innerbottommargin=\baselineskip,
    innerrightmargin=20pt,
    innerleftmargin=20pt,
    backgroundcolor=green!20!white}


\title{ROSVM Package - Mathematical Background}
\author{Eric Bach}

\begin{document}

\maketitle

\tableofcontents

\section{ToDo}

\begin{itemize}
    \item Add derivations for the exterior product features.
\end{itemize}

\section{Introduction}

This documents describes the mathematical background of the Ranking Support Vector Machine (RankSVM) \parencite{Joachims2002} implemented in the ROSVM package. 

\section{Method}

\subsection{Notation}
\begin{table}[t]
    \centering
    \caption{Notation table}
    \vspace{2pt}
    \label{tab:notations}
    \begin{tabular}{ll}
        \toprule 
        {\bf Notation} & {\bf Description} \\ \midrule
        $\Pset$ & Set of preferences with size $\numpair=|\Pset|$ \\
        $m\in\Molspace$ & Molecule from the space of molecules $\Molspace$\\
        $\x\in\mathbb{R}^{d_x}$ & Molecule feature representation, e.g. fingerprint vector, with dimension $d_x$ \\
        $\z\in\mathbb{R}^{d_z}$ & Chromatographic system feature representation with dimension $d_z$ \\
        $\phi(\x)\in\mathbb{R}^{d_\mathcal{X}}$ & \emph{Kernel} feature representation of a molecule based on $\x$\\
        $\phi_i=\phi(\x_i)$ & Shorthand for the kernel feature vector of example $i$\\
        $\bPhi\in\mathbb{R}^{\numexp\times d_\mathcal{X}}$ & Kernel feature vector matrix, with $\numexp$ examples each of dimension $d_\mathcal{X}$\\
        \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab:notations} summarizes the notation used in this document.

\subsection{Ranking Support Vector Machine (RankSVM)}

The RankSVM's primal optimization problem is given as: 
\begin{equation}
\begin{split}
    \underset{\mathbf{w},\boldsymbol{\xi}}{min} 
        &\quad f(\mathbf{w},\boldsymbol{\xi}) = \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{(i,j)\in P}\xi_{ij} \\
    \st &\quad y_{ij}\mathbf{w}^T(\phi_i-\phi_j)\geq 1-\xi_{ij},\quad\forall(i,j)\in\Pset\\
        &\quad \xi_{ij} \geq 0,\quad\forall(i,j)\in\Pset,
    \label{eq:RankSVM_primal_problem}
\end{split}
\end{equation}
where $C>0$ is the regularization parameter. We define the pairwise labels as the retention time difference of the corresponding molecules, i.e. $y_{ij}\defi\sign(t_i-t_j)$. From the primal problem in Eq.~\eqref{eq:RankSVM_primal_problem} we can derive the following dual optimization problem:
\begin{equation}
\begin{split}
    \underset{\balpha}{\max} 
        &\quad g(\balpha) = \bones^T\balpha - \frac{1}{2} \balpha^T\left(\mathbf{y}\mathbf{y}^T\circ\mathbf{B}\mathbf{K}\mathbf{B}^T\right)\balpha \\ 
    \st &\quad 0\leq\alpha_{ij}\leq C,\quad\forall (i,j)\in \Pset,
    \label{eq:RankSVM_dual_problem}
\end{split}
\end{equation}
where $\mathbf{y}\in\mathbb{R}^\numexp$ is the vector of pairwise labels, and $\mathbf{B}\in\{-1,0,1\}^{\numpair\times\numexp}$ with row $p=(i,j)$ being $[\mathbf{B}]_{p\cdot}=(0,\ldots,0,\underbrace{1}_{i},0,\ldots,0,\underbrace{-1}_{j},0,\ldots,0)$. For further details refer to the work by \cite{Kuo2014}. Using the properties of the Hadamard product $\circ$ we can reformulate the function $g(\balpha)$ of the problem in Eq.~\eqref{eq:RankSVM_dual_problem} \parencite{Styan1973}:
\begin{equation}
\begin{split}
    g(\balpha) 
        &= \bones^T\balpha-\frac{1}{2} \balpha^T\left(\mathbf{y}\mathbf{y}^T\circ\mathbf{B}\mathbf{K}\mathbf{B}^T\right)\balpha \\
        &= \bones^T\balpha-\frac{1}{2} \balpha^T\left(\mathbf{D}_\mathbf{y}\mathbf{B}\mathbf{K}\mathbf{B}^T\mathbf{D}_\mathbf{y}\right)\balpha \\
        &= \bones^T\balpha-\frac{1}{2} \balpha^T\mathbf{A}\mathbf{K}\mathbf{A}^T\balpha\\
        &= \sum_{(i,j)\in\Pset}\alpha_{ij} -\frac{1}{2}\sum_{(i,j)\in\Pset}\sum_{(u,v)\in\Pset}\alpha_{ij}\alpha_{uv}y_{ij}y_{uv}\big(\ldots\\
        &\quad\quad\underbrace{\molkern(\x_i,\x_u)-\molkern(\x_i,\x_v)-\molkern(\x_j,\x_u)+\molkern(\x_j,\x_v)}_{\text{Pairwise kernel between }(i,j)\text{ and }(u,v)}\big)
        \label{eq:dual_objective_function}
\end{split}
\end{equation}
Here, $\mathbf{D}_\mathbf{y}\in\mathbb{R}^{\numpair\times\numpair}$ is a diagonal matrix storing the pairwise labels, and $\mathbf{A}\defi\mathbf{D}_\mathbf{y}\mathbf{B}\in\{-1,0,1\}^{\numpair\times\numexp}$. The matrix $\mathbf{A}$ now contains the pairwise labels as well by multiplying each row $p=(i,j)$ of $\mathbf{B}$ with $y_{ij}$, i.e. $[\mathbf{A}]_{p\cdot}=y_{ij}\cdot(0,\ldots,0,\underbrace{1}_{i},0,\ldots,0,\underbrace{-1}_{j},0,\ldots,0)$. 
\begin{mdframed}[style=codeframe]
    Check out '\texttt{\_build\_A\_matrix}' for the actual implementation of the $\mathbf{A}$-matrix construction from the data.
\end{mdframed}

\subsubsection{Optimizing the RankSVM Model Parameters}

% \begin{figure}[t]
%     \centering
\begin{algorithm}[t]
    \label{alg:frank-wolfe}
    \SetAlgoLined 
    \caption{Conditional gradient algorithm}
    Let $\balpha^{(0)}\in\mathcal{A}$\tcc*{A feasible initial dual variable.}
    \For{$k=0,\ldots,(K-1)$}{
        $\mathbf{s}\leftarrow\underset{\mathbf{s}'\in\mathcal{A}}{\arg\max}\,
            \left\langle\nabla g(\balpha^{(k)}),\mathbf{s}'\right\rangle$
            \tcc*{Solve sub-problem}
        $\gamma^{(k)}\leftarrow\frac{2}{k+2}$\tcc*{Step-size; also line-search possible}
        $\balpha^{(k+1)}\leftarrow(1-\gamma)\balpha^{(k)}+\gamma\mathbf{s}$\tcc*{Update}
    }
    $\balpha^*\leftarrow\balpha^{(K)}$\;
\end{algorithm}
% \end{figure}

We find the optimal RankSVM model $\balpha^*$ in the dual space given a training dataset $\mathcal{D}=\{(x_i,t_i)\}_{i=1}^\numexp$ using the conditional gradient algorithm \parencite{Jaggi2013}. The algorithm is shown in \ref{alg:frank-wolfe}. The feasible set is defined as $\mathcal{A}\defi\{\balpha\in\mathbb{R}^\numpair\,|\,0\leq\alpha_{ij}\leq C,\forall (i,j)\in\Pset\}$ which follows from the constraints of the dual optimization problem in Eq.~\eqref{eq:RankSVM_dual_problem}. Note that $\mathcal{A}$ is compact convex set.
\begin{mdframed}[style=codeframe]
    The function '\texttt{\_assert\_is\_feasible}' implements the feasibility check for a given $\balpha^{(k)}$ iterate.
\end{mdframed}

\paragraph{Solving the Sub-problem:}
In each iteration of Algorithm~\ref{alg:frank-wolfe} we need to solve the following linear optimization problem:
\begin{align}
    \mathbf{s}
        &=\underset{\mathbf{s}'\in\mathcal{A}}{\arg\max}\,\left\langle\nabla g(\balpha^{(k)}),\mathbf{s}'\right\rangle\\
        &=\underset{\mathbf{s}'\in\mathcal{A}}{\arg\max}\,\left\langle\underbrace{\bones-\mathbf{A}\mathbf{K}\mathbf{A}^T\balpha^{(k)}}_{\defi\mathbf{d}},\mathbf{s}'\right\rangle.\label{eq:sub_problem}
\end{align}
Eq.~\eqref{eq:sub_problem} can be solved by simply evaluating $\mathbf{d}$ and subsequently setting the components of $s\in\mathbb{R}^{\numpair}$ as:
\begin{equation}
    s_{ij}=\begin{cases}
            C&\text{if }d_{ij}>0\\
            0&\text{else}
           \end{cases}.
\end{equation}
\begin{mdframed}[style=codeframe]
    The function '\texttt{\_solve\_sub\_problem}' implements the sub problem solver.
\end{mdframed}

\paragraph{Line-search:}
The optimal step-size $\gamma^{(k)}$ can be determined by solving an univariate problem:
\begin{equation}
    \gamma_{LS}^{(k)}=\underset{\gamma\in[0,1]}{\max}\quad g\left(\balpha^{(k)}-\gamma\left(\mathbf{s}-\balpha^{(k)}\right)\right).
    \label{eq:linesearch_problem}
\end{equation}
For that, we set the derivative of \eqref{eq:linesearch_problem} to zero: 
\begin{align}
    &\frac{\partial g\left(\balpha^{(k)}-\gamma\left(\mathbf{s}-\balpha^{(k)}\right)\right)}{\partial\gamma}\\
        &=\left(\balpha^{(k)}-\gamma\left(\mathbf{s}-\balpha^{(k)}\right)\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)-\bones^T\left(\mathbf{s}-\balpha^{(k)}\right)\\
        &=\left(\balpha^{(k)}\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)
            -\gamma\left(\mathbf{s}-\balpha^{(k)}\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)
            -\bones^T\left(\mathbf{s}-\balpha^{(k)}\right)\\
        &=0
\end{align}
and solve for $\gamma$:
\begin{align}
    \gamma\left(\mathbf{s}-\balpha^{(k)}\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)
    &=\left(\balpha^{(k)}\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)
            -\bones^T\left(\mathbf{s}-\balpha^{(k)}\right)\\
    \Leftrightarrow\\
    \gamma&=\frac{\left(\balpha^{(k)}\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)
            -\bones^T\left(\mathbf{s}-\balpha^{(k)}\right)}{\left(\mathbf{s}-\balpha^{(k)}\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)}\\
    \Leftrightarrow\\
    \gamma_{LS}^{(k)}&=\frac{\left\langle\nabla g\left(\balpha^{(k)}\right),\left(\mathbf{s}-\balpha^{(k)}\right)\right\rangle}{\left(\mathbf{s}-\balpha^{(k)}\right)^T\mathbf{A}\mathbf{K}\mathbf{A}^T\left(\mathbf{s}-\balpha^{(k)}\right)}\label{eq:optimal_stepsize_with_linesearch}.
\end{align}
To ensure that $\gamma_{LS}^{(k)}$ we clip the value to the range of $[0,1]$. To evaluate the nominator in Eq.~\eqref{eq:optimal_stepsize_with_linesearch} we can reuse the value of $\mathbf{d}=\nabla g\left(\balpha^{(k)}\right)$ calculated when solving the sub-problem (see Eq.~\eqref{eq:sub_problem})

\paragraph{Determine Convergence:}
\textcite{Jaggi2013} propose to use the duality gap:
\begin{equation}
    h\left(\balpha^{(k)}\right)\defi\left\langle\nabla g\left(\balpha^{(k)}\right),\left(\mathbf{s}-\balpha^{(k)}\right)\right\rangle
\end{equation}
as convergence criteria by defining a threshold $\epsilon$ and iterating until $h(\balpha^{(k)})<\epsilon$. The ratio behind this idea is, that the duality gap is an upper bound on the difference of the current function value for $\balpha^{(k)}$ and the one of the global maximizer $\balpha^*$ of Eq.~\eqref{eq:RankSVM_primal_problem}, i.e. $h\left(\balpha^{(k)}\right)\geq g\left(\balpha^*\right)-g\left(\balpha^{(k)}\right)$\footnote{Note: Here we formulate the dual optimization as a maximization. In \cite{Jaggi2013} the authors formulate it as a minimization which leads to slightly changed duality gap definition.}. 
\begin{mdframed}[style=openquestion]
    In practice it the duality gap $h\left(\balpha^{(k)}\right)$ was observed to have very large values and never approaching a reasonable threshold. However, the model performance was nevertheless very good. That might be because the $\balpha^{(k)}$ has many entries and those can have values up to $C$. A quadratic function of the dual vector, like $g$ in Eq.~\eqref{eq:RankSVM_dual_problem}, can take on very large values. The ``un-boundedness'' or missing normalization might be an issue here.  
    
    The current implementation gets around this issue, by checking the following quantity for the convergence:
\begin{equation}
    \frac{h(\balpha^{(k)})}{h(\balpha^{(0)})}<\epsilon,
\end{equation}
    where $\epsilon>0$, e.g. $\epsilon=0.005$, is the convergence threshold. Possible scaling factors of the function $h$ are canceled out. This convergence criteria can be interpreted as the relative decrease of the duality gap given the initial model $\balpha^{(0)}$. 
\end{mdframed}

\subsubsection{Prediction Step}
 
Given two molecules $m_u$ and $m_v$ their retention order label $\hat{y}_{uv}$ using a trained RankSVM model $\w=\bPhi^T\A^T\balpha$:
\begin{equation}
    \hat{y}_{uv}
        =\sign(\langle\w,\phi_u-\phi_v\rangle)
        =\sign\left(\left\langle\bPhi^T\A^T\balpha,\phi_u-\phi_v\right\rangle\right).
\end{equation}
Sometimes, it might be of use, to get the decision score for a molecule. This score can be calculated by exploiting the linearity of the kernel feature vector \emph{difference}
\begin{equation}
    \langle\w,\phi_u-\phi_v\rangle=\langle\w,\phi_u\rangle-\langle\w,\phi_v\rangle.
\end{equation}
We can now evaluate the decision score: 
\begin{align}
    s_u
        &=\langle\w,\phi_u\rangle\\
        &=\left\langle\bPhi^T\A^T\balpha,\phi_u\right\rangle\\
        &=\balpha^T\A\bmolkern(\x_u)\\
        &=\sum_{(i,j)\in\Pset}\alpha_{ij}y_{ij}\left(\molkern(\x_i,\x_u)-\molkern(\x_j,\x_u)\right),
\end{align}
with $\bmolkern(\x_u)=(\molkern(\x_1,\x_u),\ldots,\molkern(\x_\numexp,\x_u))\in\mathbb{R}^{\numexp}$ being a vector containing the kernel similarities between the molecule $m_u$ (using its representation $\x_u$) and all molecules in the training set. 

\subsection{Include Chromatographic System Descriptors}

In this section we are inspecting different ways to include feature descriptions of the utilized chromatographic system into the prediction. We will thereby focus on the inclusion using a joint feature-vector for molecules and chromatographic systems. 

In the following we assume, that for all pairs in the training set, i.e. $(i,j)\in\Pset$, the molecules $m_i$ and $m_j$ have been measured with the same chromatographic system. That means, no cross-system pairwise relations are explicitly measured. It furthermore motivates the use of $\z_{ij}=\z_i=\z_i$ as the notation for the chromatographic system feature descriptor corresponding to the pair $(i,j)$. 

\subsubsection{Concatenating $\x$ and $\z$}

Concatenating the feature descriptors of the molecules and the chromatographic system is one option to include the descriptors in to prediction. For that, the kernel feature vector $\phi_i$ (see Eq.~\eqref{eq:RankSVM_primal_problem}) is defined as:
\begin{equation}
    \phi_i = \phi\left(\left[\begin{matrix}\x_i\\\z_i\end{matrix}\right]\right).
\end{equation}
The \emph{pairwise kernel} (see Eq.~\eqref{eq:dual_objective_function}) will be given as:
\begin{equation}
     \molkern\left(\left[\begin{matrix}\x_i\\\z_{ij}\end{matrix}\right],\left[\begin{matrix}\x_u\\\z_{uv}\end{matrix}\right]\right)
    -\molkern\left(\left[\begin{matrix}\x_i\\\z_{ij}\end{matrix}\right],\left[\begin{matrix}\x_v\\\z_{uv}\end{matrix}\right]\right)
    -\molkern\left(\left[\begin{matrix}\x_j\\\z_{ij}\end{matrix}\right],\left[\begin{matrix}\x_u\\\z_{uv}\end{matrix}\right]\right)
    +\molkern\left(\left[\begin{matrix}\x_j\\\z_{ij}\end{matrix}\right],\left[\begin{matrix}\x_v\\\z_{uv}\end{matrix}\right]\right).
\end{equation}
Here, the kernel $\molkern$ does not have the interpretation of a similarity measure between molecules, but rather between the combination of molecule and chromatographic system features. 
\begin{mdframed}[style=openquestion]
     In practice we could for example concatenate molecular fingerprints and eluent descriptors. As kernel we can utilize the generalized Tanimoto kernel developed by \textcite{Szedmak2020a}. This kernel can be used on real valued features.
\end{mdframed}

\subsubsection{Kronecker Product of Kernel Feature vectors ($\phi(\x)\otimes\varphi(\z)$)}

Another approach to include the chromatographic system features is through a separate kernel and the Kronecker product. We define the $\phi(\x_i)\otimes\varphi(\z_i)$ as the feature associated with $\x_i$ and $\z_i$. The pairwise kernel (see Eq.~\eqref{eq:dual_objective_function}) will be given as:
\begin{align}
    &\langle\phi_i\otimes\varphi_{ij}-\phi_j\otimes\varphi_{ij},\phi_u\otimes\varphi_{uv}-\phi_v\otimes\varphi_{uv}\rangle\\
        &=\langle\phi_i\otimes\varphi_{ij},\phi_u\otimes\varphi_{uv}\rangle
         -\langle\phi_i\otimes\varphi_{ij},\phi_v\otimes\varphi_{uv}\rangle
         -\langle\phi_j\otimes\varphi_{ij},\phi_u\otimes\varphi_{uv}\rangle
         +\langle\phi_j\otimes\varphi_{ij},\phi_v\otimes\varphi_{uv}\rangle\\
        &=\molkern(\x_i,\x_u)\syskern(\z_{ij},\z_{uv})
         -\molkern(\x_i,\x_v)\syskern(\z_{ij},\z_{uv})
         -\molkern(\x_j,\x_u)\syskern(\z_{ij},\z_{uv})
         +\molkern(\x_j,\x_v)\syskern(\z_{ij},\z_{uv})\\
        &=\syskern(\z_{ij},\z_{uv})(\molkern(\x_i,\x_u)-\molkern(\x_i,\x_v)-\molkern(\x_j,\x_u)+\molkern(\x_j,\x_v)).
\end{align}
Here, $\syskern(\z_{ij},\z_{uv})$ expresses the similarity between the two chromatographic systems associated with $(i,j)$ and $(u,v)$.

\subsubsection{Chromatographic System Descriptors}



\newpage
\printbibliography

\end{document}
